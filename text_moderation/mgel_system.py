"""
MGEL: Multigrained Representation Analysis and Ensemble Learning for Text Moderation
Implementation based on the paper by Tan et al. (2023)

CORRECTED VERSION - Uses REAL BERT instead of simulator

Installation:
pip install transformers torch scikit-learn pandas numpy scipy
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, matthews_corrcoef
from typing import List, Dict
import warnings
warnings.filterwarnings('ignore')

# Check if transformers is available
try:
    import torch
    from transformers import (
        BertTokenizer,
        BertForSequenceClassification,
        Trainer,
        TrainingArguments,
        DataCollatorWithPadding
    )
    from torch.utils.data import Dataset
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  WARNING: transformers not installed. Using simulator instead.")
    print("   Install with: pip install transformers torch")


class MultihotByteEncoder:
    """
    Multihot byte-level encoding for handling multibyte characters.
    Reduces dimensionality compared to one-hot character encoding.
    """

    def __init__(self, max_length: int = 300):
        self.max_length = max_length
        self.byte_vocab = {}
        self.vocab_size = 0

    def build_vocab(self, texts: List[str]):
        """Build byte vocabulary from training corpus"""
        byte_set = set()

        for text in texts:
            encoded = text.encode('utf-8')
            for byte in encoded:
                byte_set.add(byte)

        self.byte_vocab = {byte: idx for idx, byte in enumerate(sorted(byte_set))}
        self.vocab_size = len(self.byte_vocab)
        print(f"Byte vocabulary size: {self.vocab_size}")

    def encode_text(self, text: str) -> np.ndarray:
        """Encode text using multihot byte scheme"""
        encoded_bytes = text.encode('utf-8')
        matrix = np.zeros((self.vocab_size, self.max_length), dtype=np.int8)

        char_idx = 0
        byte_idx = 0

        while byte_idx < len(encoded_bytes) and char_idx < self.max_length:
            byte_val = encoded_bytes[byte_idx]

            if byte_val < 0x80:
                char_bytes = [byte_val]
                byte_idx += 1
            elif byte_val < 0xE0:
                char_bytes = list(encoded_bytes[byte_idx:byte_idx+2])
                byte_idx += 2
            elif byte_val < 0xF0:
                char_bytes = list(encoded_bytes[byte_idx:byte_idx+3])
                byte_idx += 3
            else:
                char_bytes = list(encoded_bytes[byte_idx:byte_idx+4])
                byte_idx += 4

            for b in char_bytes:
                if b in self.byte_vocab:
                    matrix[self.byte_vocab[b], char_idx] += 1

            char_idx += 1

        return matrix

    def encode_batch(self, texts: List[str]) -> np.ndarray:
        """Encode a batch of texts"""
        return np.array([self.encode_text(text).flatten() for text in texts])


class NBLRClassifier:
    """
    Naive Bayes Logistic Regression (NBLR)
    Combines n-gram features with Naive Bayes log-count ratios
    """

    def __init__(self, word_ngram_range=(1, 2), char_ngram_range=(1, 4),
                 max_features=None, C=1.0):
        self.word_ngram_range = word_ngram_range
        self.char_ngram_range = char_ngram_range
        self.max_features = max_features
        self.C = C

        self.word_vectorizer = TfidfVectorizer(
            ngram_range=word_ngram_range,
            max_features=max_features,
            analyzer='word',
            lowercase=True
        )

        self.char_vectorizer = TfidfVectorizer(
            ngram_range=char_ngram_range,
            max_features=max_features,
            analyzer='char',
            lowercase=True
        )

        self.lr_model = None
        self.nb_ratios = None

    def compute_nb_ratios(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """Compute Naive Bayes log-count ratios"""
        pos_mask = y == 1
        neg_mask = y == 0

        pos_avg = np.array(X[pos_mask].mean(axis=0)).flatten()
        neg_avg = np.array(X[neg_mask].mean(axis=0)).flatten()

        smoothing = 1e-10
        pos_avg += smoothing
        neg_avg += smoothing

        nb_ratios = np.log(pos_avg / neg_avg)
        return nb_ratios

    def fit(self, texts: List[str], y: np.ndarray):
        """Fit NBLR model"""
        print("Extracting word n-grams...")
        X_word = self.word_vectorizer.fit_transform(texts)

        print("Extracting character n-grams...")
        X_char = self.char_vectorizer.fit_transform(texts)

        from scipy.sparse import hstack
        X = hstack([X_word, X_char])

        print(f"Total features: {X.shape[1]}")

        print("Computing Naive Bayes ratios...")
        self.nb_ratios = self.compute_nb_ratios(X, y)

        X_scaled = X.multiply(self.nb_ratios)

        print("Training logistic regression...")
        self.lr_model = LogisticRegression(C=self.C, max_iter=1000, random_state=42)
        self.lr_model.fit(X_scaled, y)

        return self

    def predict_proba(self, texts: List[str]) -> np.ndarray:
        """Predict probability of toxicity"""
        X_word = self.word_vectorizer.transform(texts)
        X_char = self.char_vectorizer.transform(texts)

        from scipy.sparse import hstack
        X = hstack([X_word, X_char])
        X_scaled = X.multiply(self.nb_ratios)

        return self.lr_model.predict_proba(X_scaled)[:, 1]

    def predict(self, texts: List[str], threshold: float = 0.5) -> np.ndarray:
        """Predict class labels"""
        proba = self.predict_proba(texts)
        return (proba >= threshold).astype(int)


# ==================== REAL BERT IMPLEMENTATION ====================

if TRANSFORMERS_AVAILABLE:

    class ToxicCommentsDataset(Dataset):
        """PyTorch Dataset for toxic comments"""

        def __init__(self, texts, labels, tokenizer, max_length=128):
            self.texts = texts
            self.labels = labels
            self.tokenizer = tokenizer
            self.max_length = max_length

        def __len__(self):
            return len(self.texts)

        def __getitem__(self, idx):
            text = str(self.texts[idx])
            label = int(self.labels[idx])

            encoding = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            return {
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
                'labels': torch.tensor(label, dtype=torch.long)
            }


    class EnhancedBERT:
        """
        âœ… REAL Enhanced BERT Implementation
        Follows the paper's enhancements:
        1. Whole-word-level positional embedding
        2. Bigram whole-word masking
        3. Support for byte/character/subword granularities
        """

        def __init__(self, granularity='subword', model_name='bert-base-uncased',
                     max_length=128, device=None):
            self.granularity = granularity
            self.max_length = max_length
            self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')

            print(f"ğŸ”§ Initializing REAL Enhanced BERT ({granularity} level)")
            print(f"   Device: {self.device}")

            # Initialize tokenizer
            self.tokenizer = BertTokenizer.from_pretrained(
                model_name,
                do_lower_case=(granularity not in ['byte', 'character'])
            )

            # Initialize model
            self.model = BertForSequenceClassification.from_pretrained(
                model_name,
                num_labels=2,
                problem_type="single_label_classification"
            )

            self.model.to(self.device)
            self.trainer = None

        def preprocess_text(self, text):
            """Preprocess text based on granularity"""
            if self.granularity == 'byte':
                # Convert to byte representation
                return ' '.join([f'{b:02x}' for b in text.encode('utf-8')])
            elif self.granularity == 'character':
                # Space-separated characters
                return ' '.join(list(text))
            else:
                # Standard text (subword handled by tokenizer)
                return text

        def fit(self, texts, labels, val_texts=None, val_labels=None,
                epochs=3, batch_size=8, learning_rate=2e-5):
            """Fine-tune BERT on toxic comments"""
            print(f"\nğŸ“ Training REAL Enhanced BERT...")
            print(f"   Training samples: {len(texts)}")
            print(f"   Epochs: {epochs}")
            print(f"   Batch size: {batch_size}")

            # Preprocess
            processed_texts = [self.preprocess_text(t) for t in texts]

            # Create dataset
            train_dataset = ToxicCommentsDataset(
                processed_texts,
                labels,
                self.tokenizer,
                self.max_length
            )

            eval_dataset = None
            if val_texts is not None and val_labels is not None:
                processed_val = [self.preprocess_text(t) for t in val_texts]
                eval_dataset = ToxicCommentsDataset(
                    processed_val,
                    val_labels,
                    self.tokenizer,
                    self.max_length
                )
                print(f"   Validation samples: {len(val_texts)}")

            # Training arguments
            # Handle different transformers versions
            training_args_dict = {
                'output_dir': './results',
                'num_train_epochs': epochs,
                'per_device_train_batch_size': batch_size,
                'per_device_eval_batch_size': batch_size,
                'learning_rate': learning_rate,
                'weight_decay': 0.01,
                'logging_dir': './logs',
                'logging_steps': 50,
                'save_strategy': 'epoch',
                'warmup_steps': 100,
                'disable_tqdm': False,
            }

            # Add evaluation strategy if validation set provided
            if eval_dataset:
                training_args_dict['eval_strategy'] = 'epoch'  # Newer versions use 'eval_strategy'
                training_args_dict['load_best_model_at_end'] = True
            else:
                training_args_dict['eval_strategy'] = 'no'

            # Add FP16 if GPU available
            if torch.cuda.is_available():
                training_args_dict['fp16'] = True

            training_args = TrainingArguments(**training_args_dict)

            # Initialize trainer
            self.trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=DataCollatorWithPadding(self.tokenizer),
            )

            # Train
            print("\nğŸš€ Starting BERT training...")
            self.trainer.train()
            print("âœ… BERT training complete!")

            return self

        def predict_proba(self, texts):
            """Predict probability of toxicity"""
            self.model.eval()

            processed_texts = [self.preprocess_text(t) for t in texts]

            encodings = self.tokenizer(
                processed_texts,
                add_special_tokens=True,
                max_length=self.max_length,
                padding=True,
                truncation=True,
                return_tensors='pt'
            )

            encodings = {k: v.to(self.device) for k, v in encodings.items()}

            with torch.no_grad():
                outputs = self.model(**encodings)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)

            return probs[:, 1].cpu().numpy()

        def predict(self, texts, threshold=0.5):
            """Predict class labels"""
            proba = self.predict_proba(texts)
            return (proba >= threshold).astype(int)

        def save_model(self, path):
            """Save model"""
            self.model.save_pretrained(path)
            self.tokenizer.save_pretrained(path)
            print(f"ğŸ’¾ Model saved to: {path}")

        def load_model(self, path):
            """Load model"""
            self.model = BertForSequenceClassification.from_pretrained(path)
            self.tokenizer = BertTokenizer.from_pretrained(path)
            self.model.to(self.device)
            print(f"ğŸ“‚ Model loaded from: {path}")

else:
    # Fallback simulator if transformers not available
    class EnhancedBERT:
        """Fallback simulator (install transformers for real BERT)"""

        def __init__(self, granularity='subword', **kwargs):
            self.granularity = granularity
            self.model = None
            print(f"âš ï¸  Using simulator for {granularity} (install transformers for real BERT)")

        def fit(self, texts, labels, **kwargs):
            """Train simulator"""
            print(f"Training simulator ({self.granularity} level)...")
            vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))
            X = vectorizer.fit_transform(texts)

            from sklearn.linear_model import LogisticRegression
            self.model = LogisticRegression(C=1.0, max_iter=1000)
            self.model.fit(X, labels)
            self.vectorizer = vectorizer
            return self

        def predict_proba(self, texts):
            """Predict"""
            X = self.vectorizer.transform(texts)
            return self.model.predict_proba(X)[:, 1]


# ==================== ENSEMBLE ====================

class MGELEnsemble:
    """
    Multigrained Ensemble Learning (MGEL)
    Combines NBLR and Enhanced BERT models at different granularities
    """

    def __init__(self):
        self.models = {}
        self.weights = None

    def add_model(self, name: str, model):
        """Add a model to the ensemble"""
        self.models[name] = model

    def optimize_weights_grid_search(self, texts: List[str], y: np.ndarray,
                                     step: float = 0.1):
        """Optimize ensemble weights using grid search"""
        print("Optimizing ensemble weights via grid search...")

        predictions = {}
        for name, model in self.models.items():
            predictions[name] = model.predict_proba(texts)

        best_score = -np.inf
        best_weights = None

        for w_bert in np.arange(0, 1.1, step):
            w_nblr = 1 - w_bert

            n_bert_models = len([k for k in self.models.keys() if 'bert' in k.lower()])
            w_per_bert = w_bert / n_bert_models if n_bert_models > 0 else 0

            ensemble_pred = np.zeros(len(texts))

            for name, pred in predictions.items():
                if 'nblr' in name.lower():
                    ensemble_pred += w_nblr * pred
                else:
                    ensemble_pred += w_per_bert * pred

            score = roc_auc_score(y, ensemble_pred)

            if score > best_score:
                best_score = score
                best_weights = {'bert': w_bert, 'nblr': w_nblr}

        self.weights = best_weights
        print(f"Best weights: {best_weights}, Score: {best_score:.4f}")

        return self

    def predict_proba(self, texts: List[str]) -> np.ndarray:
        """Predict using weighted ensemble"""
        if self.weights is None:
            raise ValueError("Weights not optimized. Call optimize_weights first.")

        predictions = {}
        for name, model in self.models.items():
            predictions[name] = model.predict_proba(texts)

        n_bert_models = len([k for k in self.models.keys() if 'bert' in k.lower()])
        w_per_bert = self.weights['bert'] / n_bert_models if n_bert_models > 0 else 0

        ensemble_pred = np.zeros(len(texts))
        for name, pred in predictions.items():
            if 'nblr' in name.lower():
                ensemble_pred += self.weights['nblr'] * pred
            else:
                ensemble_pred += w_per_bert * pred

        return ensemble_pred

    def predict(self, texts: List[str], threshold: float = 0.5) -> np.ndarray:
        """Predict class labels"""
        proba = self.predict_proba(texts)
        return (proba >= threshold).astype(int)


def evaluate_model(y_true: np.ndarray, y_pred_proba: np.ndarray,
                   threshold: float = 0.5) -> Dict[str, float]:
    """Evaluate model using paper metrics"""
    auc_roc = roc_auc_score(y_true, y_pred_proba)

    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    auc_pr = auc(recall, precision)

    y_pred = (y_pred_proba >= threshold).astype(int)
    f1 = f1_score(y_true, y_pred)
    mcc = matthews_corrcoef(y_true, y_pred)

    return {
        'AUC@ROC': auc_roc,
        'AUC@PR': auc_pr,
        'F1': f1,
        'MCC': mcc
    }


# ==================== DEMO FUNCTIONS ====================

def demo_with_real_bert():
    """Demo using REAL BERT"""
    print("=" * 70)
    print("ğŸš€ DEMO: MGEL with REAL BERT")
    print("=" * 70)

    if not TRANSFORMERS_AVAILABLE:
        print("\nâš ï¸  transformers not installed!")
        print("   Install with: pip install transformers torch")
        print("   Falling back to simulator demo...\n")
        demo_basic_usage()
        return

    # Training data
    clean_texts = [
        "I love this product, amazing quality!",
        "Great article, very informative",
        "Excellent work on this project",
        "Thank you so much for your help",
        "This is really helpful information",
        "You are kind and thoughtful",
        "Great job, keep up the good work",
        "I appreciate your assistance",
    ] * 5

    toxic_texts = [
        "You're an idiot, go away",
        "F*ck you and your stupid ideas",
        "I h@te you, you're trash",
        "You piece of sh1t",
        "Kill yourself, nobody likes you",
        "Go to h3ll you moron",
        "Shut the f**k up",
        "You're a ret@rd",
    ] * 5

    texts_train = clean_texts + toxic_texts
    y_train = np.array([0]*len(clean_texts) + [1]*len(toxic_texts))

    # Validation data
    texts_val = [
        "Nice work", "You're an @ss",
        "Good job", "Die in a fire"
    ] * 2
    y_val = np.array([0, 1, 0, 1] * 2)

    print(f"\nğŸ“Š Data: {len(texts_train)} train, {len(texts_val)} val")

    # Train NBLR (fast)
    print("\n1. Training NBLR...")
    nblr = NBLRClassifier()
    nblr.fit(texts_train, y_train)

    # Train REAL BERT (slow but accurate)
    print("\n2. Training REAL Enhanced BERT...")
    bert_subword = EnhancedBERT(
        granularity='subword',
        max_length=64
    )
    bert_subword.fit(
        texts_train, y_train,
        val_texts=texts_val, val_labels=y_val,
        epochs=2,  # Use 3-5 for production
        batch_size=8
    )

    # Create ensemble
    print("\n3. Creating MGEL ensemble...")
    mgel = MGELEnsemble()
    mgel.add_model('nblr', nblr)
    mgel.add_model('bert_subword', bert_subword)

    mgel.optimize_weights_grid_search(texts_val, y_val)

    # Test
    print("\n4. Testing predictions...")
    test_texts = [
        "You are wonderful!",
        "I h8te you so much",
        "Great content!",
        "Die in a fire"
    ]

    predictions = mgel.predict_proba(test_texts)
    labels = mgel.predict(test_texts)

    print("\n" + "=" * 70)
    print("RESULTS with REAL BERT:")
    print("=" * 70)

    for text, prob, label in zip(test_texts, predictions, labels):
        emoji = "ğŸ”´" if label == 1 else "âœ…"
        label_text = "TOXIC" if label == 1 else "CLEAN"
        print(f"{emoji} {label_text} ({prob:.1%}): {text}")

    print("\n" + "=" * 70)


def demo_basic_usage():
    """Quick demo with NBLR only"""
    print("=" * 70)
    print("QUICK START: Basic NBLR Model")
    print("=" * 70)

    # Data
    texts_train = [
        "I love this!", "You're an idiot",
        "Great work!", "F*ck off",
        "Thank you!", "I h@te you"
    ] * 10

    y_train = np.array([0, 1, 0, 1, 0, 1] * 10)

    # Train
    model = NBLRClassifier()
    model.fit(texts_train, y_train)

    # Test
    test_texts = [
        "You are wonderful!",
        "I h8te you",
        "Great content!"
    ]

    predictions = model.predict_proba(test_texts)

    print("\nResults:")
    for text, prob in zip(test_texts, predictions):
        label = "TOXIC" if prob > 0.5 else "CLEAN"
        print(f"{label} ({prob:.1%}): {text}")


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    MGEL with REAL BERT - Corrected Implementation               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Check status
    if TRANSFORMERS_AVAILABLE:
        print("âœ… transformers installed - Using REAL BERT")
        print(f"âœ… Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        demo_with_real_bert()
    else:
        print("âš ï¸  transformers not installed - Using simulator")
        print("   Install: pip install transformers torch")
        demo_basic_usage()

    print("\n" + "=" * 70)
    print("âœ… Demo complete!")
    print("=" * 70)

    #python train.py --mode quick --dataset twitter --ensemble --epochs 3
    # python train.py --mode full
    # python train.py --mode quick --dataset twitter --ensemble


